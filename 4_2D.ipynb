{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJtzkjpXluf4",
    "outputId": "efc12165-c5a6-48d8-adc6-e62fa6fac762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\mitta\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from gym) (4.8.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from gym) (1.20.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "EveClQKDTU5l",
    "outputId": "ad74aeff-ea20-4b64-866e-c2d3e2a0ba8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tools\n",
      "  Downloading tools-0.1.9.tar.gz (34 kB)\n",
      "Collecting pytils\n",
      "  Downloading pytils-0.4.1.tar.gz (99 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from tools) (1.16.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from tools) (4.6.3)\n",
      "Building wheels for collected packages: tools, pytils\n",
      "  Building wheel for tools (setup.py): started\n",
      "  Building wheel for tools (setup.py): finished with status 'done'\n",
      "  Created wheel for tools: filename=tools-0.1.9-py3-none-any.whl size=46756 sha256=856bd06f53080249ce4e591abe6cf65cf6d33e87813bf040442c1fadfd61d49e\n",
      "  Stored in directory: c:\\users\\mitta\\appdata\\local\\pip\\cache\\wheels\\77\\13\\56\\879617e6017b5dde1711ca8c9ee5e9838e913c85ae22fd96c9\n",
      "  Building wheel for pytils (PEP 517): started\n",
      "  Building wheel for pytils (PEP 517): finished with status 'done'\n",
      "  Created wheel for pytils: filename=pytils-0.4.1-py3-none-any.whl size=32538 sha256=c982a37c10c27a03901a39b4a1e8a43ece29c287a03799fdd9c634d3c4f0edea\n",
      "  Stored in directory: c:\\users\\mitta\\appdata\\local\\pip\\cache\\wheels\\4c\\26\\52\\703c9dfe2a23403a38a6d26ffde44a0f4ddb14a889d4332197\n",
      "Successfully built tools pytils\n",
      "Installing collected packages: pytils, tools\n",
      "Successfully installed pytils-0.4.1 tools-0.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3F7V9BTTSPEF",
    "outputId": "f9032b71-b774-4a20-928e-74b576d324d1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tools\n",
    "import gym\n",
    "env=gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery = False,\n",
    "             render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions required to execute Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hBZKp8Vyn1bp"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# this function computes the state values after following a certain policy\n",
    "##################\n",
    "\n",
    "def getValueFunction(env,valueFnc,policy,discount,maxIterations,convergenceTolerance):\n",
    "    import numpy as np\n",
    "    for iterations in range(maxIterations):\n",
    "        valueFncNextIteration=np.zeros(env.observation_space.n)\n",
    "        for state in env.P:\n",
    "            outerSum=0\n",
    "            for action in env.P[state]:\n",
    "                innerSum=0\n",
    "                for p, sPrime, r, isTerminalState in env.P[state][action]:\n",
    "                    #print(p, sPrime, r, isTerminalState)\n",
    "                    innerSum=innerSum+ p*(r+discount*valueFnc[sPrime])\n",
    "                outerSum=outerSum+policy[state,action]*innerSum\n",
    "            valueFncNextIteration[state]=outerSum\n",
    "        if(np.max(np.abs(valueFncNextIteration-valueFnc))<convergenceTolerance):\n",
    "            valueFnc=valueFncNextIteration\n",
    "            print('Iterative policy evaluation algorithm converged!')\n",
    "            break\n",
    "        valueFnc=valueFncNextIteration       \n",
    "    return valueFnc\n",
    "\n",
    "##################\n",
    "# this function computes an improved policy \n",
    "##################\n",
    "\n",
    "def improvePolicy(env,valueFnc,numberActions,numberStates,discount):\n",
    "    import numpy as np\n",
    "    # this matrix will store the action value functions for every state\n",
    "    qvalues=np.zeros((numberStates,numberActions))\n",
    "    # this is the improved policy\n",
    "    improvedPolicy=np.zeros((numberStates,numberActions))\n",
    "    \n",
    "    for state in range(numberStates):\n",
    "      \n",
    "        for action in range(numberActions):\n",
    "         \n",
    "            for probability, nextState, reward, isTerminalState in env.P[state][action]:\n",
    "                qvalues[state,action]=qvalues[state,action]+\\\n",
    "                probability*(reward+discount*valueFnc[nextState])\n",
    "            \n",
    "      \n",
    "        bestaction=np.where(qvalues[state,:]==np.max(qvalues[state,:]))\n",
    "\n",
    "              \n",
    "        improvedPolicy[state,bestaction]=1/np.size(bestaction)\n",
    "    return improvedPolicy,qvalues\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining our learning parameters and starting the Policy Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhl5vpwHn18u",
    "outputId": "85a7aa8b-11a1-46a9-a075-3d61aafe8a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 0 - of policy iteration algorithm\n",
      "Iterative policy evaluation algorithm converged!\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]] [2.90223704e-05 4.72195488e-05 9.58609158e-05 2.07480985e-04\n",
      " 4.46035147e-04 8.39638740e-04 1.39440631e-03 1.83941632e-03\n",
      " 2.46452501e-05 3.87578561e-05 7.66472371e-05 1.74220077e-04\n",
      " 4.91196687e-04 1.05419672e-03 2.12696418e-03 3.10531293e-03\n",
      " 1.78992668e-05 2.45670438e-05 3.26985639e-05 0.00000000e+00\n",
      " 5.10195447e-04 1.22963132e-03 3.90191335e-03 6.73267788e-03\n",
      " 1.30254322e-05 2.03862101e-05 4.45472226e-05 1.33184348e-04\n",
      " 5.47526272e-04 0.00000000e+00 7.25448654e-03 1.61854110e-02\n",
      " 6.96446051e-06 8.81188460e-06 1.19812818e-05 0.00000000e+00\n",
      " 1.79036147e-03 5.00818516e-03 1.21558973e-02 4.17641070e-02\n",
      " 2.36375548e-06 0.00000000e+00 0.00000000e+00 5.70927461e-04\n",
      " 2.40159964e-03 8.31255223e-03 0.00000000e+00 1.15513720e-01\n",
      " 1.25927026e-06 0.00000000e+00 3.29638853e-05 1.35874320e-04\n",
      " 0.00000000e+00 2.95349708e-02 0.00000000e+00 3.56116848e-01\n",
      " 2.00743300e-06 3.66790426e-06 1.06340887e-05 0.00000000e+00\n",
      " 3.56963239e-02 1.22954009e-01 3.58276969e-01 0.00000000e+00]\n",
      "Iteration - 1 - of policy iteration algorithm\n",
      "Iterative policy evaluation algorithm converged!\n",
      "[[0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]] [0.25418658 0.28242954 0.3138106  0.34867844 0.38742049 0.43046721\n",
      " 0.4782969  0.531441   0.28242954 0.3138106  0.34867844 0.38742049\n",
      " 0.43046721 0.4782969  0.531441   0.59049    0.25418658 0.28242954\n",
      " 0.3138106  0.         0.4782969  0.531441   0.59049    0.6561\n",
      " 0.34867844 0.38742049 0.43046721 0.4782969  0.531441   0.\n",
      " 0.6561     0.729      0.3138106  0.34867844 0.38742049 0.\n",
      " 0.59049    0.6561     0.729      0.81       0.28242954 0.\n",
      " 0.         0.59049    0.6561     0.729      0.         0.9\n",
      " 0.25418658 0.         0.4782969  0.531441   0.         0.81\n",
      " 0.         1.         0.34867844 0.38742049 0.43046721 0.\n",
      " 0.81       0.9        1.         0.        ]\n",
      "Iteration - 2 - of policy iteration algorithm\n",
      "Iterative policy evaluation algorithm converged!\n",
      "[[0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]] [0.25418658 0.28242954 0.3138106  0.34867844 0.38742049 0.43046721\n",
      " 0.4782969  0.531441   0.28242954 0.3138106  0.34867844 0.38742049\n",
      " 0.43046721 0.4782969  0.531441   0.59049    0.3138106  0.34867844\n",
      " 0.38742049 0.         0.4782969  0.531441   0.59049    0.6561\n",
      " 0.34867844 0.38742049 0.43046721 0.4782969  0.531441   0.\n",
      " 0.6561     0.729      0.3138106  0.34867844 0.38742049 0.\n",
      " 0.59049    0.6561     0.729      0.81       0.28242954 0.\n",
      " 0.         0.59049    0.6561     0.729      0.         0.9\n",
      " 0.3138106  0.         0.4782969  0.531441   0.         0.81\n",
      " 0.         1.         0.34867844 0.38742049 0.43046721 0.\n",
      " 0.81       0.9        1.         0.        ]\n",
      "Iteration - 3 - of policy iteration algorithm\n",
      "Iterative policy evaluation algorithm converged!\n",
      "[[0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.   0.5 ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]] [0.25418658 0.28242954 0.3138106  0.34867844 0.38742049 0.43046721\n",
      " 0.4782969  0.531441   0.28242954 0.3138106  0.34867844 0.38742049\n",
      " 0.43046721 0.4782969  0.531441   0.59049    0.3138106  0.34867844\n",
      " 0.38742049 0.         0.4782969  0.531441   0.59049    0.6561\n",
      " 0.34867844 0.38742049 0.43046721 0.4782969  0.531441   0.\n",
      " 0.6561     0.729      0.3138106  0.34867844 0.38742049 0.\n",
      " 0.59049    0.6561     0.729      0.81       0.28242954 0.\n",
      " 0.         0.59049    0.6561     0.729      0.         0.9\n",
      " 0.3138106  0.         0.4782969  0.531441   0.         0.81\n",
      " 0.         1.         0.34867844 0.38742049 0.43046721 0.\n",
      " 0.81       0.9        1.         0.        ]\n",
      "Policy iteration algorithm converged!\n"
     ]
    }
   ],
   "source": [
    "discount=0.9\n",
    "\n",
    "stateNumber=env.observation_space.n\n",
    "\n",
    "actionNumber=env.action_space.n\n",
    "\n",
    "maxPolicyIteration=1000\n",
    "\n",
    "initialPolicy=(1/actionNumber)*np.ones((stateNumber,actionNumber))\n",
    "\n",
    "valueFncInitial=np.zeros(env.observation_space.n)\n",
    "\n",
    "maxPolicyEvaluation=1000\n",
    "\n",
    "converganceTolerance=10**(-6)\n",
    "###########################################################################\n",
    "\n",
    "for iteration in range(maxPolicyIteration):\n",
    "    print(\"Iteration - {} - of policy iteration algorithm\".format(iteration))\n",
    "    if (iteration == 0):\n",
    "        currentPolicy=initialPolicy\n",
    "    valueFnccomputed =getValueFunction(env,valueFncInitial,currentPolicy,\n",
    "                                       discount,maxPolicyEvaluation,converganceTolerance)\n",
    "    print(currentPolicy, valueFnccomputed)\n",
    "    improvedPolicy,qvalues=improvePolicy(env,valueFnccomputed,\n",
    "                                         actionNumber,stateNumber,discount)\n",
    " \n",
    "    # if two policies are similar up to a certain \"small\" tolerance\n",
    "    if np.allclose(currentPolicy,improvedPolicy):\n",
    "        currentPolicy=improvedPolicy\n",
    "        print(\"Policy iteration algorithm converged!\")\n",
    "        break\n",
    "    currentPolicy=improvedPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to convert our environment to an MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nXz-AxhPEoLG"
   },
   "outputs": [],
   "source": [
    "def convert_frozenlake_to_mdp(env):\n",
    "    # Extract the necessary information from the environment\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    transition_matrix = np.zeros((num_states, num_actions, num_states))\n",
    "    reward_matrix = np.zeros((num_states, num_actions))\n",
    "\n",
    "    # Build the transition and reward matrices\n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            for transition in env.P[state][action]:\n",
    "                next_state, probability, reward, _ = transition\n",
    "                transition_matrix[state, action, next_state] = probability\n",
    "                reward_matrix[state, action] = reward\n",
    "    \n",
    "    \n",
    "    mdp = MDP(transition_matrix, reward_matrix, initial_state='0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to make mdp to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQQqOeQqGlOm",
    "outputId": "9bcddbd7-7063-4b3e-92e4-da4471185bcc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pexpect/popen_spawn.py:60: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  self._read_thread.setDaemon(True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    " from google.colab import drive\n",
    " import sys\n",
    " drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILB0WtpFGrDp",
    "outputId": "ecf34b51-4286-487f-ed11-5a0678ad9672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mdp/__init__.py\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/')\n",
    "import mdp as mdp_file\n",
    "print(mdp_file.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "tg-_f7wJF9LZ",
    "outputId": "ed8fdecf-2ece-49d1-8e66-ee58c0723484"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ec5c6d9bb477>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmdp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MDP' from 'mdp' (/usr/local/lib/python3.10/dist-packages/mdp/__init__.py)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from mdp import MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppSP1DKzE-z6",
    "outputId": "029ff050-a965-4dcd-fd44-e915aa3a112a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: mdp in /usr/local/lib/python3.10/dist-packages (3.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mdp) (1.22.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from mdp) (0.18.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install mdp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "lhIyfqv8FdvH",
    "outputId": "a08cbf67-fd9d-41ed-94ed-969590b14537"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-fdabf1591a59>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmdp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconvert_frozenlake_to_mdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MDP' from 'mdp' (/usr/local/lib/python3.10/dist-packages/mdp/__init__.py)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from mdp import MDP\n",
    "\n",
    "convert_frozenlake_to_mdp(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yacdJuAGFadD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
